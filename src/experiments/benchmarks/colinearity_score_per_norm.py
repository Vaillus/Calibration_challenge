#!/usr/bin/env python3
"""
Analyse de la colin√©arit√© des flow vectors par bins de normes.

Ce script analyse la relation entre la norme des flow vectors et leur score de colin√©arit√©
avec le vecteur qui pointe vers le point de fuite (label).

Usage:
    python colinearity_by_norm_analysis.py --video_id 3 --max_frames 200
"""

import numpy as np
import matplotlib.pyplot as plt
import argparse
from pathlib import Path
from typing import Optional, Dict, List, Tuple
import seaborn as sns
import mlx.core as mx

from src.utilities.load_ground_truth import read_ground_truth_pixels
from src.utilities.paths import get_flows_dir, get_outputs_dir
from src.core.collinearity_scorer_batch import BatchCollinearityScorer

def load_flows_and_labels(video_id: int, max_frames: Optional[int] = None) -> Tuple[Optional[mx.array], Optional[mx.array]]:
    """
    Charge les flows et les labels pour une vid√©o donn√©e, retourne des arrays MLX.
    
    Args:
        video_id: ID de la vid√©o
        max_frames: Nombre maximum de frames √† charger (None = toutes)
        
    Returns:
        Tuple (flows_mx, labels_mx) ou (None, None) si erreur
    """
    # Charger les flows
    npz_path = get_flows_dir() / f"{video_id}_float16.npz"
    if npz_path.exists():
        print(f"üìÇ Chargement flows vid√©o {video_id} (format NPZ)...")
        with np.load(npz_path) as data:
            flows_np = data['flow']
            if max_frames is not None:
                flows_np = flows_np[:max_frames]
            flows_mx = mx.array(flows_np, dtype=mx.float32)
            print(f"‚úÖ Flows charg√©s: {flows_mx.shape}")
    else:
        # Essayer le format original
        npy_path = get_flows_dir() / f"{video_id}.npy"
        if npy_path.exists():
            print(f"üìÇ Chargement flows vid√©o {video_id} (format NPY)...")
            flows_np = np.load(npy_path)
            if max_frames is not None:
                flows_np = flows_np[:max_frames]
            flows_mx = mx.array(flows_np, dtype=mx.float32)
            print(f"‚úÖ Flows charg√©s: {flows_mx.shape}")
        else:
            print(f"‚ùå Fichier de flows non trouv√© pour vid√©o {video_id}")
            return None, None
    
    # Charger les labels
    try:
        print(f"üìÇ Chargement labels vid√©o {video_id}...")
        labels_np = read_ground_truth_pixels(video_id)
        # Ajuster les indices (skiper la premi√®re frame)
        labels_np = labels_np[1:]  
        if max_frames is not None:
            labels_np = labels_np[:max_frames]
        labels_mx = mx.array(labels_np, dtype=mx.float32)
        print(f"‚úÖ Labels charg√©s: {labels_mx.shape}")
        return flows_mx, labels_mx
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des labels: {e}")
        return None, None

def define_adaptive_norm_bins(norms: np.ndarray, focus_on_large: bool = True) -> Dict:
    """
    D√©finit des bins avec granularit√© adaptative - plus de d√©tail sur les grands vecteurs.
    
    Args:
        norms: Array des normes (all frames flattened)
        focus_on_large: Si True, plus de granularit√© sur les grands vecteurs
        
    Returns:
        Dictionnaire avec les d√©finitions des bins
    """
    print(f"üìä D√©finition de bins adaptatifs (focus sur les grands vecteurs)...")
    
    # Exclure les normes nulles
    non_zero_norms = norms[norms > 1e-10]
    
    # Calculer des percentiles cl√©s
    p25 = np.percentile(non_zero_norms, 25)
    p40 = np.percentile(non_zero_norms, 40)
    p60 = np.percentile(non_zero_norms, 60)
    p75 = np.percentile(non_zero_norms, 75)
    p85 = np.percentile(non_zero_norms, 85)
    p92 = np.percentile(non_zero_norms, 92)
    p96 = np.percentile(non_zero_norms, 96)
    p99 = np.percentile(non_zero_norms, 99)
    max_norm = np.max(non_zero_norms)
    
    if focus_on_large:
        # Regrouper les petits, plus de granularit√© sur les grands
        bins = {
            'petits': {
                'min': 0.0,
                'max': float(p40),  # Regroupe tr√®s_petites + petites
                'percentile_min': 0,
                'percentile_max': 40,
                'count': 0
            },
            'moyens': {
                'min': float(p40),
                'max': float(p60),
                'percentile_min': 40,
                'percentile_max': 60,
                'count': 0
            },
            'grands': {
                'min': float(p60),
                'max': float(p75),
                'percentile_min': 60,
                'percentile_max': 75,
                'count': 0
            },
            'tr√®s_grands': {
                'min': float(p75),
                'max': float(p85),
                'percentile_min': 75,
                'percentile_max': 85,
                'count': 0
            },
            'ultra_grands': {
                'min': float(p85),
                'max': float(p92),
                'percentile_min': 85,
                'percentile_max': 92,
                'count': 0
            },
            'mega_grands': {
                'min': float(p92),
                'max': float(p96),
                'percentile_min': 92,
                'percentile_max': 96,
                'count': 0
            },
            'extr√™mes': {
                'min': float(p96),
                'max': float(max_norm),
                'percentile_min': 96,
                'percentile_max': 100,
                'count': 0
            }
        }
    else:
        # Granularit√© √©quilibr√©e
        percentiles = [0, 20, 40, 60, 80, 100]
        thresholds = [0.0] + [np.percentile(non_zero_norms, p) for p in percentiles[1:]]
        bin_names = ['petits', 'moyens', 'grands', 'tr√®s_grands', 'extr√™mes']
        
        bins = {}
        for i, name in enumerate(bin_names):
            bins[name] = {
                'min': float(thresholds[i]),
                'max': float(thresholds[i + 1]),
                'percentile_min': percentiles[i],
                'percentile_max': percentiles[i + 1],
                'count': 0
            }
    
    # Affichage des bins
    print("üéØ Bins d√©finis (granularit√© adaptative):")
    for name, bin_info in bins.items():
        print(f"  ‚Ä¢ {name:12s}: [{bin_info['min']:6.3f}, {bin_info['max']:6.3f}] "
              f"(P{bin_info['percentile_min']:3.0f}-P{bin_info['percentile_max']:3.0f})")
    
    return bins

def compute_colinearity_by_bins_parallel(flows_mx: mx.array, labels_mx: mx.array, bins: Dict, batch_size: int = 50) -> Dict:
    """
    Calcule les scores de colin√©arit√© pour chaque bin de normes en utilisant le traitement parall√®le MLX.
    
    Args:
        flows_mx: Array MLX des flows (n_frames, h, w, 2)
        labels_mx: Array MLX des points de fuite (n_frames, 2)
        bins: D√©finition des bins
        batch_size: Taille des batches pour le traitement
        
    Returns:
        Dictionnaire avec les r√©sultats par bin
    """
    print("üî¢ Calcul des scores de colin√©arit√© par bins (version parall√®le)...")
    
    # Initialiser l'estimateur parall√®le
    estimator = BatchCollinearityScorer(
        frame_width=int(flows_mx.shape[2]),
        frame_height=int(flows_mx.shape[1])
    )
    
    results = {}
    for bin_name in bins.keys():
        results[bin_name] = {
            'colinearity_scores': [],
            'count': 0,
            'mean_score': 0.0,
            'std_score': 0.0,
            'median_score': 0.0
        }
    
    n_frames = flows_mx.shape[0]
    print(f"   Traitement de {n_frames} frames par batches de {batch_size}...")
    
    # Calculer toutes les normes en une fois
    print("   Calcul des normes pour toutes les frames...")
    flow_x = flows_mx[:, :, :, 0]
    flow_y = flows_mx[:, :, :, 1]
    all_norms = mx.sqrt(flow_x**2 + flow_y**2)  # Shape: (n_frames, h, w)
    mx.eval(all_norms)
    
    # Traiter par batches
    for start_idx in range(0, n_frames, batch_size):
        end_idx = min(start_idx + batch_size, n_frames)
        if (end_idx - 1) % 50 == 0 or end_idx == n_frames:
            print(f"   Batch {start_idx}-{end_idx-1}")
        
        # Extraire le batch
        flows_batch = flows_mx[start_idx:end_idx]
        labels_batch = labels_mx[start_idx:end_idx]
        norms_batch = all_norms[start_idx:end_idx]
        
        # Calculer les cartes de colin√©arit√© pour tout le batch
        colinearity_maps = []
        for i in range(flows_batch.shape[0]):
            flow_frame = flows_batch[i]
            label_point = (float(labels_batch[i, 0]), float(labels_batch[i, 1]))
            colinearity_map = estimator.compute_colinearity_map(np.array(flow_frame), label_point)
            colinearity_maps.append(colinearity_map)
        
        # Convertir en array MLX pour traitement vectoris√©
        colinearity_batch = mx.stack(colinearity_maps)  # Shape: (batch_size, h, w)
        mx.eval(colinearity_batch)
        
        # Analyser par bins pour ce batch
        for bin_name, bin_info in bins.items():
            # Masque pour les pixels dans ce bin pour tout le batch
            bin_mask = (norms_batch >= bin_info['min']) & (norms_batch < bin_info['max'])
            
            # Si c'est le dernier bin, inclure la borne sup√©rieure
            if bin_name == list(bins.keys())[-1]:
                bin_mask = (norms_batch >= bin_info['min']) & (norms_batch <= bin_info['max'])
            
            # Exclure les pixels sans mouvement
            motion_mask = norms_batch > 1e-10
            final_mask = bin_mask & motion_mask
            
            # Extraire les scores de colin√©arit√© pour ce bin
            if mx.sum(final_mask) > 0:
                # Convertir en numpy pour l'indexage bool√©en
                final_mask_np = np.array(final_mask)
                colinearity_batch_np = np.array(colinearity_batch)
                bin_scores_np = colinearity_batch_np[final_mask_np]
                results[bin_name]['colinearity_scores'].extend(bin_scores_np.tolist())
                results[bin_name]['count'] += len(bin_scores_np)
    
    # Calculer les statistiques finales
    print("üìä Calcul des statistiques par bin...")
    for bin_name, result in results.items():
        if len(result['colinearity_scores']) > 0:
            scores = np.array(result['colinearity_scores'])
            result['mean_score'] = float(np.mean(scores))
            result['std_score'] = float(np.std(scores))
            result['median_score'] = float(np.median(scores))
            result['count'] = len(scores)
            
            # Ajouts pour analyse plus pouss√©e
            result['percentiles'] = {
                'p10': float(np.percentile(scores, 10)),
                'p25': float(np.percentile(scores, 25)),
                'p75': float(np.percentile(scores, 75)),
                'p90': float(np.percentile(scores, 90))
            }
        else:
            print(f"‚ö†Ô∏è Aucun pixel trouv√© pour le bin {bin_name}")
    
    return results

def analyze_colinearity_patterns(bins: Dict, results: Dict) -> Dict:
    """
    Analyse les patterns dans les scores de colin√©arit√©.
    
    Args:
        bins: D√©finition des bins
        results: R√©sultats par bin
        
    Returns:
        Dictionnaire avec l'analyse des patterns
    """
    print("üîç Analyse des patterns de colin√©arit√©...")
    
    analysis = {
        'bin_summary': [],
        'best_bin': None,
        'worst_bin': None,
        'correlation_norm_colinearity': 0.0,
        'recommendations': []
    }
    
    # Cr√©er un r√©sum√© par bin
    valid_bins = []
    for bin_name, bin_info in bins.items():
        if bin_name in results and results[bin_name]['count'] > 0:
            result = results[bin_name]
            summary = {
                'name': bin_name,
                'norm_range': (bin_info['min'], bin_info['max']),
                'count': result['count'],
                'mean_score': result['mean_score'],
                'median_score': result['median_score'],
                'std_score': result['std_score']
            }
            analysis['bin_summary'].append(summary)
            valid_bins.append(summary)
    
    if len(valid_bins) == 0:
        print("‚ö†Ô∏è Aucun bin valide trouv√©")
        return analysis
    
    # Trouver le meilleur et le pire bin
    analysis['best_bin'] = max(valid_bins, key=lambda x: x['mean_score'])
    analysis['worst_bin'] = min(valid_bins, key=lambda x: x['mean_score'])
    
    # Analyser la corr√©lation entre norme et colin√©arit√©
    if len(valid_bins) >= 3:
        norm_centers = [(b['norm_range'][0] + b['norm_range'][1]) / 2 for b in valid_bins]
        mean_scores = [b['mean_score'] for b in valid_bins]
        correlation = np.corrcoef(norm_centers, mean_scores)[0, 1]
        analysis['correlation_norm_colinearity'] = float(correlation)
    
    # G√©n√©rer des recommandations
    if analysis['correlation_norm_colinearity'] > 0.5:
        analysis['recommendations'].append("Les vecteurs de grande norme sont mieux align√©s")
    elif analysis['correlation_norm_colinearity'] < -0.5:
        analysis['recommendations'].append("Les vecteurs de petite norme sont mieux align√©s")
    else:
        analysis['recommendations'].append("Pas de corr√©lation claire entre norme et alignement")
    
    if analysis['best_bin']['mean_score'] > 0.7:
        analysis['recommendations'].append(f"Excellent alignement dans le bin '{analysis['best_bin']['name']}'")
    elif analysis['best_bin']['mean_score'] < 0.3:
        analysis['recommendations'].append("Alignement g√©n√©ral faible - v√©rifier la qualit√© des donn√©es")
    
    return analysis

def visualize_results(bins: Dict, results: Dict, analysis: Dict, video_id: int):
    """
    Visualise les r√©sultats de l'analyse.
    
    Args:
        bins: D√©finition des bins
        results: R√©sultats par bin
        analysis: Analyse des patterns
        video_id: ID de la vid√©o
    """
    print("üìà Cr√©ation des visualisations...")
    
    plt.style.use('default')
    sns.set_palette("husl")
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle(f'Analyse colin√©arit√© par normes - Vid√©o {video_id}', fontsize=16, fontweight='bold')
    
    # Pr√©parer les donn√©es pour les graphiques
    valid_bins = [s for s in analysis['bin_summary'] if s['count'] > 0]
    if not valid_bins:
        print("‚ö†Ô∏è Pas de donn√©es valides pour la visualisation")
        return
    
    bin_names = [b['name'] for b in valid_bins]
    mean_scores = [b['mean_score'] for b in valid_bins]
    counts = [b['count'] for b in valid_bins]
    norm_ranges = [f"{b['norm_range'][0]:.2f}-{b['norm_range'][1]:.2f}" for b in valid_bins]
    
    # 1. Scores moyens par bin
    ax1 = axes[0, 0]
    bars1 = ax1.bar(range(len(bin_names)), mean_scores, alpha=0.7)
    ax1.set_xlabel('Bins de normes')
    ax1.set_ylabel('Score de colin√©arit√© moyen')
    ax1.set_title('Score de colin√©arit√© moyen par bin')
    ax1.set_xticks(range(len(bin_names)))
    ax1.set_xticklabels(bin_names, rotation=45)
    ax1.grid(True, alpha=0.3)
    
    # Colorier le meilleur bin
    if analysis['best_bin']:
        best_idx = next(i for i, b in enumerate(valid_bins) if b['name'] == analysis['best_bin']['name'])
        bars1[best_idx].set_color('green')
        bars1[best_idx].set_alpha(0.9)
    
    # 2. Distribution des comptes par bin
    ax2 = axes[0, 1]
    ax2.bar(range(len(bin_names)), counts, alpha=0.7, color='orange')
    ax2.set_xlabel('Bins de normes')
    ax2.set_ylabel('Nombre de pixels')
    ax2.set_title('Nombre de pixels par bin')
    ax2.set_xticks(range(len(bin_names)))
    ax2.set_xticklabels(bin_names, rotation=45)
    ax2.grid(True, alpha=0.3)
    
    # 3. Box plots des distributions
    ax3 = axes[1, 0]
    scores_lists = []
    box_labels = []
    for bin_name in bin_names:
        if bin_name in results and len(results[bin_name]['colinearity_scores']) > 0:
            scores = results[bin_name]['colinearity_scores']
            # Sous-√©chantillonner si trop de donn√©es
            if len(scores) > 10000:
                scores = np.random.choice(scores, 10000, replace=False)
            scores_lists.append(scores)
            box_labels.append(bin_name)
    
    if scores_lists:
        bp = ax3.boxplot(scores_lists, labels=box_labels, patch_artist=True)
        colors = plt.cm.Set3(np.linspace(0, 1, len(bp['boxes'])))
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
        ax3.set_xlabel('Bins de normes')
        ax3.set_ylabel('Score de colin√©arit√©')
        ax3.set_title('Distribution des scores par bin')
        ax3.grid(True, alpha=0.3)
    
    # 4. Corr√©lation norme vs colin√©arit√©
    ax4 = axes[1, 1]
    if len(valid_bins) >= 3:
        norm_centers = [(b['norm_range'][0] + b['norm_range'][1]) / 2 for b in valid_bins]
        ax4.scatter(norm_centers, mean_scores, s=100, alpha=0.7)
        
        # Ligne de tendance
        z = np.polyfit(norm_centers, mean_scores, 1)
        p = np.poly1d(z)
        x_trend = np.linspace(min(norm_centers), max(norm_centers), 100)
        ax4.plot(x_trend, p(x_trend), "r--", alpha=0.8)
        
        ax4.set_xlabel('Centre du bin (norme)')
        ax4.set_ylabel('Score de colin√©arit√© moyen')
        ax4.set_title(f'Corr√©lation: r={analysis["correlation_norm_colinearity"]:.3f}')
        ax4.grid(True, alpha=0.3)
        
        # Annoter les points
        for i, name in enumerate(bin_names):
            ax4.annotate(name, (norm_centers[i], mean_scores[i]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.tight_layout()
    
    # Sauvegarder
    output_path = get_outputs_dir() / f"colinearity_by_norm_analysis_video_{video_id}.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"üìä Graphiques sauvegard√©s: {output_path}")
    
    plt.show()

def print_detailed_report(bins: Dict, results: Dict, analysis: Dict, video_id: int):
    """
    Affiche un rapport d√©taill√© de l'analyse.
    
    Args:
        bins: D√©finition des bins
        results: R√©sultats par bin
        analysis: Analyse des patterns
        video_id: ID de la vid√©o
    """
    print(f"\n{'='*80}")
    print(f"RAPPORT D'ANALYSE COLIN√âARIT√â PAR NORMES - VID√âO {video_id}")
    print(f"{'='*80}")
    
    print(f"\nüìä R√âSUM√â PAR BIN:")
    print(f"{'Bin':12} {'Range':15} {'Count':>8} {'Mean':>6} {'Median':>6} {'Std':>6}")
    print(f"{'-'*60}")
    
    for summary in analysis['bin_summary']:
        print(f"{summary['name']:12} "
              f"[{summary['norm_range'][0]:5.2f},{summary['norm_range'][1]:5.2f}] "
              f"{summary['count']:>8,} "
              f"{summary['mean_score']:>6.3f} "
              f"{summary['median_score']:>6.3f} "
              f"{summary['std_score']:>6.3f}")
    
    print(f"\nüéØ ANALYSE DES PATTERNS:")
    if analysis['best_bin']:
        print(f"  ‚Ä¢ Meilleur alignement: {analysis['best_bin']['name']} "
              f"(score moyen: {analysis['best_bin']['mean_score']:.3f})")
    
    if analysis['worst_bin']:
        print(f"  ‚Ä¢ Pire alignement: {analysis['worst_bin']['name']} "
              f"(score moyen: {analysis['worst_bin']['mean_score']:.3f})")
    
    print(f"  ‚Ä¢ Corr√©lation norme ‚Üî colin√©arit√©: {analysis['correlation_norm_colinearity']:.3f}")
    
    if abs(analysis['correlation_norm_colinearity']) > 0.3:
        trend = "positive" if analysis['correlation_norm_colinearity'] > 0 else "n√©gative"
        strength = "forte" if abs(analysis['correlation_norm_colinearity']) > 0.7 else "mod√©r√©e"
        print(f"    ‚Üí Corr√©lation {strength} {trend}")
    else:
        print(f"    ‚Üí Pas de corr√©lation significative")
    
    print(f"\nüí° RECOMMANDATIONS:")
    for rec in analysis['recommendations']:
        print(f"  ‚Ä¢ {rec}")
    
    # Recommandations pour le filtrage
    print(f"\nüîß IMPLICATIONS POUR LE FILTRAGE:")
    if analysis['correlation_norm_colinearity'] > 0.5:
        print(f"  ‚Ä¢ Privil√©gier les vecteurs de grande norme dans le filtrage")
        print(f"  ‚Ä¢ Seuil recommand√©: ‚â• {analysis['best_bin']['norm_range'][0]:.2f}")
    elif analysis['correlation_norm_colinearity'] < -0.5:
        print(f"  ‚Ä¢ Les vecteurs de petite norme sont plus fiables")
        print(f"  ‚Ä¢ √âviter les seuils trop √©lev√©s")
    else:
        print(f"  ‚Ä¢ La norme seule n'est pas un bon crit√®re de filtrage")
        print(f"  ‚Ä¢ Consid√©rer d'autres crit√®res (colin√©arit√© directe, etc.)")

def main():
    parser = argparse.ArgumentParser(description="Analyse de la colin√©arit√© par bins de normes")
    parser.add_argument('--video_id', type=int, default=3, help='ID de la vid√©o √† analyser')
    parser.add_argument('--max_frames', type=int, default=100, help='Nombre max de frames √† analyser')
    parser.add_argument('--focus_large', action='store_true', default=True, help='Granularit√© fine sur les grands vecteurs')
    parser.add_argument('--no_plot', action='store_true', help='Ne pas afficher les graphiques')
    
    args = parser.parse_args()
    
    print(f"üöÄ ANALYSE COLIN√âARIT√â PAR NORMES (GRANULARIT√â ADAPTATIVE)")
    print(f"Vid√©o: {args.video_id}")
    print(f"Frames: {args.max_frames if args.max_frames else 'toutes'}")
    print(f"Granularit√© sur grands vecteurs: {'OUI' if args.focus_large else 'NON'}")
    print(f"{'='*60}")
    
    # Charger les donn√©es (maintenant en MLX)
    flows_mx, labels_mx = load_flows_and_labels(args.video_id, args.max_frames)
    if flows_mx is None or labels_mx is None:
        return
    
    # Calculer les normes avec MLX (plus efficace)
    print("üî¢ Calcul des normes des flow vectors (MLX)...")
    flow_x = flows_mx[:, :, :, 0]
    flow_y = flows_mx[:, :, :, 1]
    all_norms_mx = mx.sqrt(flow_x**2 + flow_y**2)
    mx.eval(all_norms_mx)
    
    # Convertir en numpy pour d√©finir les bins (besoin des percentiles)
    flat_norms = np.array(all_norms_mx.flatten())
    print(f"‚úÖ Normes calcul√©es pour {len(flat_norms):,} pixels")
    
    # D√©finir les bins avec granularit√© adaptative
    bins = define_adaptive_norm_bins(flat_norms, focus_on_large=args.focus_large)
    
    # Calculer la colin√©arit√© par bins (version parall√®le optimis√©e)
    results = compute_colinearity_by_bins_parallel(flows_mx, labels_mx, bins, batch_size=50)
    
    # Analyser les patterns
    analysis = analyze_colinearity_patterns(bins, results)
    
    # Afficher le rapport
    print_detailed_report(bins, results, analysis, args.video_id)
    
    # Visualiser
    if not args.no_plot:
        visualize_results(bins, results, analysis, args.video_id)
    
    print(f"\n‚úÖ Analyse termin√©e !")

if __name__ == "__main__":
    main() 